{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CharlyAlizadeh_Week2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md0b9ubyi-1S"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezXfhR8TjA8J"
      },
      "source": [
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PX2kIH2Y5-Pg"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3mBwqYjSTQ-"
      },
      "source": [
        "# Exe. 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78c6U79dFrA2"
      },
      "source": [
        "This function initialize a neural network with three layer: one input layer, one hidden layer and one output layer. The size (number of neuron) of those layers are controlled respectively by the variable `n_inputs`, `n_hidden` and `n_ouputs`. In a neural network each neuron of a layer $l$ takes $n + 1$ inputs, $n$ being the number of neurons of the previous layer. This explain the `n_inputs + 1` and `n_hidden + 1` in the following lines\n",
        "```python\n",
        "hidden_layer = [{'weights': [random.uniform(0, 1) for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
        "```\n",
        "```python\n",
        "output_layer = [{'weights': [random.uniform(0, 1) for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s0pGm71SA-K"
      },
      "source": [
        "def initialize_network(n_inputs, n_hidden, n_outputs):\r\n",
        "    network = []\r\n",
        "    hidden_layer = [{'weights': [random.uniform(0, 1) for i in range(n_inputs + 1)]} for i in range(n_hidden)]\r\n",
        "    network.append(hidden_layer)\r\n",
        "    output_layer = [{'weights': [random.uniform(0, 1) for i in range(n_hidden + 1)]} for i in range(n_outputs)]\r\n",
        "    network.append(output_layer)\r\n",
        "    return network"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2oQeWYCofWO"
      },
      "source": [
        "# Exe. 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTZWWUFuiy0C",
        "outputId": "1a93659e-f4ea-4c01-c2e8-50059081034a"
      },
      "source": [
        "random.seed(75321)\n",
        "n_inputs = 2\n",
        "n_hidden = 5\n",
        "n_outputs = 2\n",
        "network = initialize_network(n_inputs, n_hidden, n_outputs)\n",
        "print(f\"Size of the layers:\\n  Input: {n_inputs}\\n  Hidden: {n_hidden}\\n  Outputs: {n_outputs}\")\n",
        "print(\"Hidden layer\")\n",
        "for neuron in network[0]:\n",
        "    print(neuron)\n",
        "\n",
        "print(\"Output layer\")\n",
        "for neuron in network[1]:\n",
        "    print(neuron)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of the layers:\n",
            "  Input: 2\n",
            "  Hidden: 5\n",
            "  Outputs: 2\n",
            "Hidden layer\n",
            "{'weights': [0.9714766128877328, 0.04582540515793432, 0.4647321330399077]}\n",
            "{'weights': [0.04978621609922318, 0.7915577777862831, 0.6379848841414051]}\n",
            "{'weights': [0.7095852423252691, 0.73374145259484, 0.22648915307454343]}\n",
            "{'weights': [0.22010270992783676, 0.7662678562071541, 0.05277171170978456]}\n",
            "{'weights': [0.5462838393848582, 0.46195571576425065, 0.24452164960776868]}\n",
            "Output layer\n",
            "{'weights': [0.5389771150897168, 0.9499511289168537, 0.15955823694744276, 0.3395969140118491, 0.902364324662603, 0.05435740173015913]}\n",
            "{'weights': [0.760969225018772, 0.05216026210717284, 0.4095126570294081, 0.7215625050532741, 0.8841398107571936, 0.9201530464750866]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rmlKEXFsEfB"
      },
      "source": [
        "# Exe. 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxeb47zyHHOG"
      },
      "source": [
        "The process of going through a neuron is done in two steps: **activation** and **transfere**. The activation part conscists into computing the linear combination of the outputs of the previous layer with their respective weights.\n",
        "\n",
        "Let's $i^l_k$ and $o^l_k$ be respectively the input at a neuron before and after applying the activation function with $l$ the layer and $k$ the neuron position in its layer.\n",
        "\n",
        "Let also $w_{ab}$ be the weight from the neuron $a$ to $b$ ($a$ and $b$ belonging to two consecutives layers). If $a = 0$ then $w_{0b}$ represents the bias at the neuron $b$.\n",
        "\n",
        "We have:\n",
        "$$\n",
        "i^l_k = \\sum\\limits_{z} o^{l-1}_z \\cdot w_{zk} + w_{0k}\n",
        "$$\n",
        "\n",
        "We can also express this expression for a whole layer thanks to matrices.\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "1\\\\\n",
        "o^{l-1}_1 \\\\\n",
        "o^{l-1}_2 \\\\\n",
        "\\vdots \\\\\n",
        "o^{l-1}_m\n",
        "\\end{pmatrix}\n",
        "\\cdot\n",
        "\\begin{pmatrix}\n",
        "w_{01} & w_{11} & w_{12} & \\cdots & w_{1m}\\\\\n",
        "w_{02} & w_{21} & w_{22} & \\cdots & w_{2m}\\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
        "w_{0n} & w_{n1} & w_{n2} & \\cdots & w_{nm}\n",
        "\\end{pmatrix}=\n",
        "\\begin{pmatrix}\n",
        "i^l_1\\\\\n",
        "i^l_2\\\\\n",
        "\\vdots\\\\\n",
        "i^l_n\n",
        "\\end{pmatrix}\\\\\n",
        " \\Leftrightarrow o^{l-1} \\cdot w_{l,l-1}  = i^l\n",
        "$$\n",
        "with $m$ being the the size of the layer $l-1$ and $n$ the size of the layer $l$. The $1$ in the vector of the layer $l-1$ denotes the bias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT27KTntsGVr"
      },
      "source": [
        "def activate(weights, inputs):\n",
        "    activation = sum([weights[i] * inputs[i] for i in range(len(weights) - 1)]) + weights[-1]\n",
        "    return activation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubrPkXvlzVNU"
      },
      "source": [
        "# Exe. 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLopVkzGQUiY"
      },
      "source": [
        "The three activation functions proposed in this TD are the *sigmoid*, *tanh* and *relu* functions.\n",
        "\n",
        "$$\n",
        "sigmoid(x) = \\frac{1}{1 + e^{-x}}\\\\\n",
        "tanh(x) = \\frac{1-e^{-2z}}{1+e^{-2z}}\\\\\n",
        "relu(x) = \\left\\{\\begin{array}{lll}0&if&x\\leq0\\\\x&else&\\end{array}\\right.\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOR48vWxzX70"
      },
      "source": [
        "# Sigmoïd\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + math.exp(-x))\n",
        "\n",
        "# Tanh\n",
        "def tanh(x):\n",
        "    return math.tanh(x)\n",
        "\n",
        "# Rectifier transfer (reLU)\n",
        "def relu(x):\n",
        "    return max(0, x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQgqYPve0blM"
      },
      "source": [
        "def transfer(activation, transfer_function=sigmoid):\n",
        "    return transfer_function(activation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iktuacEW3BGm"
      },
      "source": [
        "# Exe. 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONauttJhvJ8t"
      },
      "source": [
        "In the Exe. 3 we expressed the vector $i^l$ thanks to the vector $o^{l-1}$ and the weights matrix $w_{l,l-1}$. Now we want to express the vector $o^l$ in terms of the vector $i^l$.\n",
        "$$\n",
        "f(i^l) = o^l\n",
        "$$\n",
        "where the function $f$ is the activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nA2DMjT3DVu"
      },
      "source": [
        "def forward_propagate(network, row, transfer_function=sigmoid):\n",
        "    inputs = row\n",
        "    for layer in network:\n",
        "        new_inputs = []\n",
        "        for neuron in layer:\n",
        "            neuron['output'] = transfer(activate(neuron['weights'], inputs), transfer_function)\n",
        "            new_inputs.append(neuron['output'])\n",
        "        inputs = new_inputs\n",
        "    return inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F5xLgWR8363"
      },
      "source": [
        "# Exe. 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3393mXoRwwL0"
      },
      "source": [
        "Here we initialize a neural network with 4 inputs, an hidden layer composed of 8 neurons and an output layer with 2 neurons. This neural network will work as a classifier, meaning that each outputs of the network corresponds to the probability of the input to belong to a certain class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1ww8KPn6cHe",
        "outputId": "bff0f583-be89-467a-bf6a-89a16cf9ee97"
      },
      "source": [
        "network = initialize_network(4, 8, 2)\n",
        "forward_propagate(network, [random.randint(0, 1) for i in range(4)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.962237684309359, 0.9783434078370609]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kypkixYi86OG"
      },
      "source": [
        "# Exe. 7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNcrwCil9Iiv"
      },
      "source": [
        "# Sigmoïd\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Tanh\n",
        "def tanh_derivative(x):\n",
        "    return 1 - math.tanh(x)**2\n",
        "\n",
        "# Rectifier transfer\n",
        "def relu_derivative(x):\n",
        "    return 1 if x > 0 else 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmySMU4dLTDc"
      },
      "source": [
        "def transfer_derivative(output, derivative_function=sigmoid_derivative):\n",
        "    return derivative_function(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQGcLncMMbv6"
      },
      "source": [
        "# Exe. 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TX0E9aeyfN0"
      },
      "source": [
        "In the following explanation the notation changed a little bit. The input of a neuron before applying the activation is now noted $net_j$ instead of $i^l$. Also we may not specify the layer when it is obvious or not relevant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzYNZfD1A9az"
      },
      "source": [
        "## Backward propagation explaination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA-nlFRh5t5x"
      },
      "source": [
        "The backpropagation algorithm uses the following formula ([Wikipedia](https://en.wikipedia.org/wiki/Backpropagation#math_Eq._5))\n",
        "\n",
        "$\\frac{\\partial E}{\\partial w_{ij}} = o_i\\delta_j$\n",
        "\n",
        "with\n",
        "\n",
        "$\\delta_j = \\frac{\\partial E}{\\partial o_j}\\frac{\\partial o_j}{\\partial net_j}$ \n",
        "$\\left\\{\n",
        "    \\begin{array}{lr}\n",
        "        \\frac{\\partial L(o_j, t)}{\\partial o_j}\\frac{\\partial \\varphi(net_j)}{\\partial net_j} & if\\ j\\ is\\ an\\ output\\ neuron\\\\\n",
        "        (\\sum\\limits_{l\\in N} w_{jl}\\delta_l)\\frac{\\partial \\varphi(net_j)}{\\partial net_j} & if\\ j\\ is\\ an\\ inner\\ neuron\\\\\n",
        "    \\end{array} \n",
        "\\right.$\n",
        "\n",
        "with:\n",
        "* $E$ the error\n",
        "* $L$ the error function\n",
        "* $\\varphi$ the activation function\n",
        "* $N$ the set of neurons of the next layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU2OwiaVBIta"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3ulBcYbUzKn"
      },
      "source": [
        "def backward_propagate_error(network, expected):\n",
        "    for i in reversed(range(len(network))):\n",
        "        layer = network[i]\n",
        "        errors = []\n",
        "        # If layer is an hidden layer\n",
        "        if i != len(network) -1:\n",
        "            for j in range(len(layer)):\n",
        "                error = 0.0\n",
        "                for neuron in network[i + 1]:\n",
        "                    error += (neuron['weights'][j] * neuron['delta'])\n",
        "                    errors.append(error)\n",
        "        # If layer is the output layer\n",
        "        else:\n",
        "            for j in range(len(layer)):\n",
        "                neuron = layer[j]\n",
        "                errors.append(expected[j] - neuron['output'])\n",
        "        for j in range(len(layer)):\n",
        "            neuron = layer[j]\n",
        "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK1NSMJtBK3z"
      },
      "source": [
        "## Code explaination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovojWnVSBNxK"
      },
      "source": [
        "The backward propagation algorithm starts at the last layer (output) of the network and end at the first layer (input). \n",
        "```python\n",
        "for i in reverse(range(len(network))):\n",
        "```\n",
        "For each layer we have two possibility: output or hidden. (the input layer being treated as an hidden layer)\n",
        "```python\n",
        "if i != len(network) - 1:\n",
        "  # Hidden layer\n",
        "else:\n",
        "  # Output layer\n",
        "```\n",
        "\n",
        "### Finding the error\n",
        "\n",
        "For each weight we want to update it thanks to the following formula $w_{kj} = w_{kj} - \\epsilon\\frac{\\partial E}{\\partial w_{kj}}$. To do so we need to compute the value of $\\frac{\\partial E}{\\partial w_{kj}}$. Thanks to the **chain rule** we have $\\frac{\\partial E}{\\partial w_{kj}} = \\frac{\\partial E}{\\partial o_j}\\frac{\\partial o_j}{\\partial w_{kj}}$.\n",
        "\n",
        "#### 1. Computing $\\mathbf{\\frac{\\partial o_j}{\\partial w_{kj}}}$\n",
        "\n",
        "For every neuron (regardless if it's in the ouput or a hidden layer) $\\frac{\\partial o_j}{\\partial w_{kj}} = \\frac{\\partial o_j}{\\partial net_j}\\frac{\\partial net_j}{\\partial w_{kj}}$\n",
        "\n",
        "We have $\\frac{\\partial net_j}{\\partial w_{kj}} = o_k$ and $\\frac{\\partial o_j}{\\partial net_j} = \\frac{\\partial \\varphi(net_j)}{\\partial net_j}$ with $\\varphi$ being the activation function at the neuron j.\n",
        "This terms corresponds to the variable `transfer_derivative(neuron['output']` in the line.\n",
        "\n",
        "```python\n",
        "neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
        "```\n",
        "\n",
        "#### 2. Computing $\\mathbf{\\frac{\\partial E}{\\partial o_j}}$\n",
        "\n",
        "##### Output Layer\n",
        "If the layer is the ouput layer we compute the error directly thanks to the expected vector $t$. The following line corresponds to $\\frac{\\partial L(o_j,t)}{\\partial o_j}$ where $L$ is the error formula. In our case we use the **Sum Squared Error** multiplied by $\\frac{1}{2}$ (in order to get rid of the $2$ when we derive it). \n",
        "We have\n",
        "$\\frac{\\partial L(o_j, t)}{\\partial o_j} = \\frac{\\partial }{\\partial o_j}\\frac{1}{2}\\sum\\limits_n (t_n - o_n)^2 = -(t_j-o_j) = \\mathbf{(o_j - t_j)}$\n",
        "```python\n",
        "errors.append(expected[j] - neuron['output'])\n",
        "```\n",
        "\n",
        "#### Hidden Layer\n",
        "\n",
        "If the layer is in a hidden layer the error given the ouput of the neuron can be expressed by  \n",
        "$\\frac{\\partial E}{\\partial o_j} = \\sum\\limits_{l\\in N}(\\frac{\\partial E}{\\partial net_l}\\frac{\\partial net_j}{\\partial o_j}) = \\sum\\limits_{l\\in N}(\\frac{\\partial E}{\\partial o_l}\\frac{\\partial o_l}{\\partial net_l}\\frac{\\partial net_l}{\\partial o_j})$\n",
        "with $N$ being the set of neurons receiving an input from the neuron $j$ (so in our case all the neurons of the next layer).  \n",
        "$\\frac{\\partial net_l}{\\partial o_j} = \\frac{\\partial }{\\partial o_j} \\cdots + w_{jl}o_j + \\cdots = w_{jl}$ with $\\cdots$ representing parts not depending of $o_j$ and so equal to 0 when derivating.\n",
        "For a given neuron $j$ we note $\\delta_j$ the value $\\frac{\\partial E}{\\partial o_j}\\frac{\\partial o_j}{\\partial net_j}$\n",
        "\n",
        ">> Note that when computing the error caused by the weight $w_{kj}$ we use the delta of all the neuron of the next layer.\n",
        "\n",
        "So when replacing in the equation we have\n",
        "$\\frac{\\partial E}{\\partial o_j} = \\mathbf{\\sum\\limits_{l\\in N}w_{jl}\\delta_l}$\n",
        "\n",
        "```python\n",
        "for neuron in network[i + 1]:\n",
        "    error += (neuron['weights'][j] * neuron['delta'])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXCqo2RuV625"
      },
      "source": [
        "# Exe. 9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGA7BdHtWO8x"
      },
      "source": [
        "backward_propagate_error(network, [0.3, 0.7])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIkaxf8kYwEJ",
        "outputId": "b718f03a-5e5e-4bba-eade-53ce96c22195"
      },
      "source": [
        "network"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[{'delta': -0.0027781590460267402,\n",
              "   'output': 0.6306983916768553,\n",
              "   'weights': [0.012163073165286975,\n",
              "    0.41372034025118654,\n",
              "    0.23304931001963358,\n",
              "    0.4017342401504366,\n",
              "    0.10933067328175594]},\n",
              "  {'delta': -0.0026874719892457134,\n",
              "   'output': 0.8101014131887616,\n",
              "   'weights': [0.8729902888453033,\n",
              "    0.5312015471370521,\n",
              "    0.6709037033700326,\n",
              "    0.2157982174047578,\n",
              "    0.04647742927643528]},\n",
              "  {'delta': -0.001782337160206606,\n",
              "   'output': 0.825163971137449,\n",
              "   'weights': [0.3067035919244072,\n",
              "    0.8809351128643115,\n",
              "    0.45481524694203723,\n",
              "    0.43456248792609964,\n",
              "    0.3640948579340768]},\n",
              "  {'delta': -0.0024458551416968615,\n",
              "   'output': 0.8133176577065322,\n",
              "   'weights': [0.7942665292932252,\n",
              "    0.36090418324339835,\n",
              "    0.29398587133246246,\n",
              "    0.2385513996022781,\n",
              "    0.31654257547774234]},\n",
              "  {'delta': -0.00013698055241724932,\n",
              "   'output': 0.8521846332112525,\n",
              "   'weights': [0.772868480065494,\n",
              "    0.22362853061163923,\n",
              "    0.16027955707441432,\n",
              "    0.25885807366595626,\n",
              "    0.7553422250584284]},\n",
              "  {'delta': -0.001431087770358541,\n",
              "   'output': 0.6279133148792967,\n",
              "   'weights': [0.09562189932899001,\n",
              "    0.15626521954168893,\n",
              "    0.20497156814654716,\n",
              "    0.05906558322621647,\n",
              "    0.2713881527931381]},\n",
              "  {'delta': -0.001828668901332643,\n",
              "   'output': 0.8450573862324169,\n",
              "   'weights': [0.38178968911095346,\n",
              "    0.6570219466673084,\n",
              "    0.22100966877418793,\n",
              "    0.5239647944195112,\n",
              "    0.6575380873657548]},\n",
              "  {'delta': -0.0032832543817323586,\n",
              "   'output': 0.7785551145625941,\n",
              "   'weights': [0.34650943878705553,\n",
              "    0.8847422307828738,\n",
              "    0.3472515574388618,\n",
              "    0.9648239580344232,\n",
              "    0.02601438072674589]}],\n",
              " [{'delta': -0.02406328253514623,\n",
              "   'output': 0.962237684309359,\n",
              "   'weights': [0.49567762144895533,\n",
              "    0.5134093820716047,\n",
              "    0.045190875899653205,\n",
              "    0.5803942871400719,\n",
              "    0.16564633471076196,\n",
              "    0.8003659280746063,\n",
              "    0.6421494093735871,\n",
              "    0.41061067563091014,\n",
              "    0.4940204508423083]},\n",
              "  {'delta': -0.005897424384169375,\n",
              "   'output': 0.9783434078370609,\n",
              "   'weights': [0.9397261357048448,\n",
              "    0.6366570912043616,\n",
              "    0.854235600229107,\n",
              "    0.8609569822820632,\n",
              "    0.12460818066622925,\n",
              "    0.15938183699299147,\n",
              "    0.4289793563294988,\n",
              "    0.9298764447238932,\n",
              "    0.0042549271754783735]}]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtdTRP12Zoay"
      },
      "source": [
        "# Exe. 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v534iXoqSI9k"
      },
      "source": [
        "The following line corresponds to $w_{kj}^{new} = w_{kj} + \\epsilon \\frac{dE}{dw_{kj}} = w_{kj} + \\epsilon \\delta_jo_k$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AXn9uo_lMMM"
      },
      "source": [
        "def update_weight(network, row, l_rate):\n",
        "    row = row.copy() + [1] # The `[1]` is the input of the bias\n",
        "    for layer in network:    \n",
        "        next_row = []\n",
        "        for neuron in layer:\n",
        "            for i in range(len(neuron['weights'])):\n",
        "                neuron['weights'][i] += l_rate * neuron['delta'] * row[i]\n",
        "            next_row.append(neuron['output'])\n",
        "        row = next_row + [1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSFboqYh7noU"
      },
      "source": [
        "# Exe. 11\n",
        "\n",
        "$SSE(e,o) = \\sum\\limits_{i}(e_i\\cdot o_i)^2$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeYmdgmw7tQ6"
      },
      "source": [
        "def sse(expected, output):\n",
        "    if type(expected) == list and type(output) == list:\n",
        "        if len(expected) != len(output):\n",
        "            raise ValueError(f\"The length of the expected and output vector must have the same length. Got {len(expected), len(output)}\")\n",
        "        return sum([(expected[i] - output[i]) ** 2 for i in range(len(expected))])\n",
        "    else:\n",
        "        return (expected - output) ** 2\n",
        "\n",
        "\n",
        "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
        "    for epoch in range(n_epoch):\n",
        "        sum_error = 0\n",
        "        for row in train:\n",
        "            outputs = forward_propagate(network, row[:-1])\n",
        "            # The last value of the row list is the expected class, those class goes from 0 to the number of class.\n",
        "            # It's conveniant because we can express the expected vector easily by constructing a vector of 0 where the index equal to the class\n",
        "            # equal to 1.\n",
        "            expected = [0 for i in range(n_outputs)]\n",
        "            expected[int(row[-1])] = 1 \n",
        "            sum_error += sse(expected, outputs)\n",
        "            backward_propagate_error(network, expected)\n",
        "            update_weight(network, row, l_rate)\n",
        "        print(f\">epoch = {epoch}, lrate = {l_rate}, error = {sum_error}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7ubZ0N-jrth"
      },
      "source": [
        "# Exe. 12"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XoQKAlXcDgP"
      },
      "source": [
        "network = initialize_network(2, 4, 2)\n",
        "data_set = [[random.uniform(0, 10), random.uniform(0, 10), random.randint(0, 1)] for i in range(10)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cg414PLddohW",
        "outputId": "17cb274c-f7c7-4435-fe4a-96dd0fd55ced"
      },
      "source": [
        "train_network(network, data_set, 0.1, 10, 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">epoch = 0, lrate = 0.1, error = 8.477483661563202\n",
            ">epoch = 1, lrate = 0.1, error = 8.245005114201732\n",
            ">epoch = 2, lrate = 0.1, error = 7.954751873030464\n",
            ">epoch = 3, lrate = 0.1, error = 7.596664785026787\n",
            ">epoch = 4, lrate = 0.1, error = 7.170697583618001\n",
            ">epoch = 5, lrate = 0.1, error = 6.69903471358336\n",
            ">epoch = 6, lrate = 0.1, error = 6.230396346823783\n",
            ">epoch = 7, lrate = 0.1, error = 5.820546995768858\n",
            ">epoch = 8, lrate = 0.1, error = 5.501505546897389\n",
            ">epoch = 9, lrate = 0.1, error = 5.2727649538915635\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCqo6hWDjwC1"
      },
      "source": [
        "# Exe. 13"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5cd5x5DkU1u"
      },
      "source": [
        "def predict(network, row):\n",
        "    probabilities = forward_propagate(network, row)\n",
        "    # We want the index of the max probability in the predicted vector.\n",
        "    # This index corresponds to the preditcted class.\n",
        "    return probabilities.index(max(probabilities)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LG5hjjRmM7-"
      },
      "source": [
        "# Exe. 14"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-3jRTQFmOsO",
        "outputId": "a92f34c6-cf50-4e97-84b3-f5eb2c1f54ef"
      },
      "source": [
        "print([data[2] for data in data_set])\n",
        "print([predict(network, data) for data in data_set])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 1, 0, 0, 1, 0, 0, 1, 1]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goclc2yGXfO7"
      },
      "source": [
        "# Exe. 15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ienqUkQdYsyt"
      },
      "source": [
        "# Exe. 16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ASHSXM_Xhmj"
      },
      "source": [
        "def load_csv(filename, delimiter='\\t', strip='\"', tonumpy=True):\n",
        "    data = []\n",
        "    with open(filename) as file:\n",
        "        for line in file:\n",
        "            row = [float(nb.replace(strip, '')) for nb in line.split(delimiter)]\n",
        "            row[-1] = int(row[-1])\n",
        "            data.append(row)\n",
        "    return np.array(data) if tonumpy else data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBksjmXYY0lb"
      },
      "source": [
        "# Exe. 17"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh2LY4KaY36b"
      },
      "source": [
        "data_set = load_csv(\"seeds_dataset.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEldPrjpak9s"
      },
      "source": [
        "np.random.shuffle(data_set)\n",
        "max_cols = np.amax(data_set, 0)\n",
        "min_cols = np.amin(data_set, 0)\n",
        "for j in range(data_set.shape[1] - 1):\n",
        "    data_set[:, j] = (data_set[:, j] - min_cols[j]) / (max_cols[j] - min_cols[j])\n",
        "train_index = int(0.8 * data_set.shape[0])\n",
        "train_set = data_set[:train_index, :]\n",
        "test_set = data_set[train_index:, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KE_yTEcjU21",
        "outputId": "22399c2c-ef1d-497a-ceb5-bf3b53c8bfde"
      },
      "source": [
        "np.unique(data_set[:,-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 2., 3.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb8K-86HjF4u"
      },
      "source": [
        "# Exe. 18"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onCyOQDDjIP1"
      },
      "source": [
        "network = initialize_network(7, 14, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_x45qSlj8-u",
        "outputId": "9e233b72-15b7-43fb-bcf1-97ccb31f8ea2"
      },
      "source": [
        "train_network(network, train_set.tolist(), 0.1, 100, 4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">epoch = 0, lrate = 0.1, error = 358.49330561085037\n",
            ">epoch = 1, lrate = 0.1, error = 277.39700712859536\n",
            ">epoch = 2, lrate = 0.1, error = 193.5564719759718\n",
            ">epoch = 3, lrate = 0.1, error = 186.1325069402888\n",
            ">epoch = 4, lrate = 0.1, error = 185.62656151961252\n",
            ">epoch = 5, lrate = 0.1, error = 185.1204609567656\n",
            ">epoch = 6, lrate = 0.1, error = 184.59966930758168\n",
            ">epoch = 7, lrate = 0.1, error = 184.05327261967759\n",
            ">epoch = 8, lrate = 0.1, error = 183.47071921214638\n",
            ">epoch = 9, lrate = 0.1, error = 182.83981745075707\n",
            ">epoch = 10, lrate = 0.1, error = 182.14495242024026\n",
            ">epoch = 11, lrate = 0.1, error = 181.36475595848503\n",
            ">epoch = 12, lrate = 0.1, error = 180.4679524959251\n",
            ">epoch = 13, lrate = 0.1, error = 179.40317510913405\n",
            ">epoch = 14, lrate = 0.1, error = 178.06233051588086\n",
            ">epoch = 15, lrate = 0.1, error = 176.04028771359174\n",
            ">epoch = 16, lrate = 0.1, error = 153.59014215654443\n",
            ">epoch = 17, lrate = 0.1, error = 103.14746304897243\n",
            ">epoch = 18, lrate = 0.1, error = 102.00698839781187\n",
            ">epoch = 19, lrate = 0.1, error = 100.61237235088385\n",
            ">epoch = 20, lrate = 0.1, error = 99.05253051744526\n",
            ">epoch = 21, lrate = 0.1, error = 97.31841743949936\n",
            ">epoch = 22, lrate = 0.1, error = 95.41358594303763\n",
            ">epoch = 23, lrate = 0.1, error = 93.35742226224795\n",
            ">epoch = 24, lrate = 0.1, error = 91.18552951755356\n",
            ">epoch = 25, lrate = 0.1, error = 88.94600291317334\n",
            ">epoch = 26, lrate = 0.1, error = 86.69250976946446\n",
            ">epoch = 27, lrate = 0.1, error = 84.47678572078296\n",
            ">epoch = 28, lrate = 0.1, error = 82.34289905825662\n",
            ">epoch = 29, lrate = 0.1, error = 80.32415981847774\n",
            ">epoch = 30, lrate = 0.1, error = 78.44237965525194\n",
            ">epoch = 31, lrate = 0.1, error = 76.70878234209236\n",
            ">epoch = 32, lrate = 0.1, error = 75.12586659995706\n",
            ">epoch = 33, lrate = 0.1, error = 73.68963463333996\n",
            ">epoch = 34, lrate = 0.1, error = 72.39175170083973\n",
            ">epoch = 35, lrate = 0.1, error = 71.22137618495034\n",
            ">epoch = 36, lrate = 0.1, error = 70.16655827268691\n",
            ">epoch = 37, lrate = 0.1, error = 69.21521696457528\n",
            ">epoch = 38, lrate = 0.1, error = 68.3557635915628\n",
            ">epoch = 39, lrate = 0.1, error = 67.57745777586388\n",
            ">epoch = 40, lrate = 0.1, error = 66.87057604031632\n",
            ">epoch = 41, lrate = 0.1, error = 66.22645779182338\n",
            ">epoch = 42, lrate = 0.1, error = 65.63747641034392\n",
            ">epoch = 43, lrate = 0.1, error = 65.09696846737927\n",
            ">epoch = 44, lrate = 0.1, error = 64.5991427951363\n",
            ">epoch = 45, lrate = 0.1, error = 64.13898304835958\n",
            ">epoch = 46, lrate = 0.1, error = 63.71215190106922\n",
            ">epoch = 47, lrate = 0.1, error = 63.314901407585744\n",
            ">epoch = 48, lrate = 0.1, error = 62.94399175198497\n",
            ">epoch = 49, lrate = 0.1, error = 62.59661918101176\n",
            ">epoch = 50, lrate = 0.1, error = 62.27035306119188\n",
            ">epoch = 51, lrate = 0.1, error = 61.96308151845149\n",
            ">epoch = 52, lrate = 0.1, error = 61.67296487368493\n",
            ">epoch = 53, lrate = 0.1, error = 61.39839599199902\n",
            ">epoch = 54, lrate = 0.1, error = 61.13796665860503\n",
            ">epoch = 55, lrate = 0.1, error = 60.890439142210056\n",
            ">epoch = 56, lrate = 0.1, error = 60.65472218222374\n",
            ">epoch = 57, lrate = 0.1, error = 60.42985072303623\n",
            ">epoch = 58, lrate = 0.1, error = 60.21496880703537\n",
            ">epoch = 59, lrate = 0.1, error = 60.009315122041116\n",
            ">epoch = 60, lrate = 0.1, error = 59.81221077533681\n",
            ">epoch = 61, lrate = 0.1, error = 59.623048934157616\n",
            ">epoch = 62, lrate = 0.1, error = 59.441286031160104\n",
            ">epoch = 63, lrate = 0.1, error = 59.266434283495755\n",
            ">epoch = 64, lrate = 0.1, error = 59.09805531643522\n",
            ">epoch = 65, lrate = 0.1, error = 58.93575471795989\n",
            ">epoch = 66, lrate = 0.1, error = 58.77917738029753\n",
            ">epoch = 67, lrate = 0.1, error = 58.62800350891009\n",
            ">epoch = 68, lrate = 0.1, error = 58.4819451997386\n",
            ">epoch = 69, lrate = 0.1, error = 58.340743502272744\n",
            ">epoch = 70, lrate = 0.1, error = 58.204165899826\n",
            ">epoch = 71, lrate = 0.1, error = 58.072004149766784\n",
            ">epoch = 72, lrate = 0.1, error = 57.94407243578865\n",
            ">epoch = 73, lrate = 0.1, error = 57.820205791939074\n",
            ">epoch = 74, lrate = 0.1, error = 57.700258764337775\n",
            ">epoch = 75, lrate = 0.1, error = 57.58410428151426\n",
            ">epoch = 76, lrate = 0.1, error = 57.47163270825201\n",
            ">epoch = 77, lrate = 0.1, error = 57.36275106085232\n",
            ">epoch = 78, lrate = 0.1, error = 57.25738236392201\n",
            ">epoch = 79, lrate = 0.1, error = 57.15546513016988\n",
            ">epoch = 80, lrate = 0.1, error = 57.056952945292494\n",
            ">epoch = 81, lrate = 0.1, error = 56.96181413980332\n",
            ">epoch = 82, lrate = 0.1, error = 56.870031528553014\n",
            ">epoch = 83, lrate = 0.1, error = 56.781602196606855\n",
            ">epoch = 84, lrate = 0.1, error = 56.696537306962185\n",
            ">epoch = 85, lrate = 0.1, error = 56.61486190113573\n",
            ">epoch = 86, lrate = 0.1, error = 56.5366146577507\n",
            ">epoch = 87, lrate = 0.1, error = 56.46184756669781\n",
            ">epoch = 88, lrate = 0.1, error = 56.39062546703832\n",
            ">epoch = 89, lrate = 0.1, error = 56.323025385400065\n",
            ">epoch = 90, lrate = 0.1, error = 56.25913559812083\n",
            ">epoch = 91, lrate = 0.1, error = 56.199054324909625\n",
            ">epoch = 92, lrate = 0.1, error = 56.14288794471753\n",
            ">epoch = 93, lrate = 0.1, error = 56.090748606644866\n",
            ">epoch = 94, lrate = 0.1, error = 56.04275109154906\n",
            ">epoch = 95, lrate = 0.1, error = 55.99900876592642\n",
            ">epoch = 96, lrate = 0.1, error = 55.95962846221989\n",
            ">epoch = 97, lrate = 0.1, error = 55.92470412400506\n",
            ">epoch = 98, lrate = 0.1, error = 55.89430907729116\n",
            ">epoch = 99, lrate = 0.1, error = 55.8684868387874\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBWitAXhlTum"
      },
      "source": [
        "# Exe. 19"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3OhhK6P3V_t"
      },
      "source": [
        "The accuracy corresponds to the number of good prediction on the number of observation in the testing set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx5nTfSYliGW"
      },
      "source": [
        "def accuracy(expected, output):\n",
        "    return sum(1 if expected[i] == output[i] else 0 for i in range(len(expected))) / len(expected)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DH6oqy836AZ9"
      },
      "source": [
        "def mse(expected, output):\n",
        "    return sum((expected[i] - output[i])**2 for i in range(len(output))) / len(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCqRwFzSlVnu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b325771-46f9-4f0e-dffa-34e43aa4a911"
      },
      "source": [
        "expected = [data[-1] for data in test_set]\n",
        "output = [predict(network, data) for data in test_set]\n",
        "print(f'Accuracy: {accuracy(expected, output)}')\n",
        "print(f'MSE: {mse(expected, output)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8571428571428571\n",
            "MSE: 0.42857142857142855\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}